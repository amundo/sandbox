<!doctype html>
<html>
<head>
  <meta charset=utf-8>
  <title>
propagate glosses
  </title>
  <link rel=stylesheet href=beforeAfter.css></link>

</head>
<body>

<header>
<h1>
<button id=afterButton>propagate</button> glosses
</h1>
<p class=help>

</p>
</header>

<main>
<div id=before>
  <textarea spellchecker=false></textarea>
</div>

<div id=after>
  <textarea spellchecker=false></textarea>
</div>
</main>

<script>
/*
Initializes a text from the left textarea, propagates known glosses to unknown token instances,
and puts the resulting text to the right textarea.

It does this by building a text-level lexicon (a “crib”) of the  text consisting of already-glossed words
Then it goes through the phrases of the text, and if the .words array on the phrase is empty, it tokenizes 
the transcription field and looks up each word in turn. if it finds it, it appends the word to the phrase.words, 
if not it adds an empty word with the gloss as an empty string.

This sort of thing will be incorporated into various interfaces and applications, but it's a reasonable 
test bed for (for instance) working out the Lexicon API.

*/
var 
  before = document.querySelectorAll('#before'),
  beforeTA = document.querySelector('#before textarea'),

  after = document.querySelectorAll('#after'),
  afterTA = document.querySelector('#after textarea'),

  afterButton = document.querySelector('#afterButton');


function Text(text){
  this.phrases = text.phrases;

  this.crib = new Lexicon();
  
  this.extractLexicon = () => {
    return this.phrases.forEach(p => {
      p.words.forEach(word => { 
        this.crib.add(word)  
      })
    })
  }
}

function Lexicon(){
  this.words = [];

  this.add = (word) => { 
    this.words.push(word) 
  } 

  this.lookup = query => { 
    return this.words
      .filter(word => { 
         return word.token == query 
      })[0]
  } 
  
}

window.app = {};

function tokenize(text){
  return text.toLowerCase().replace('\.', ' ', 'g').split(/[ ]/);
}

function propagate(ev){
  var text = new Text(JSON.parse(beforeTA.value));
  text.extractLexicon();

  var unglossedPhrases = text.phrases.filter(p => p.words.length == 0);

  unglossedPhrases.forEach(p =>  {
    var tokens = tokenize(p.transcription);

    tokens.forEach(token =>  {
      if(text.crib.lookup(token)){
        p.words.push( text.crib.lookup(token) )
      } else { 
        p.words.push( {token: token, gloss: ''} )
      }
    })
  })

  afterTA.value = JSON.stringify(text, null, 2);
}
 

function doSomething(){
}

function convert(){
  
  var 
    text = JSON.parse(beforeTA.value); 
  
  afterTA.value = propagate(text); 

}

afterButton.addEventListener('click', propagate);
//beforeTA.addEventListener('input', convert);

</script>
</body>
</html>
